{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f0d8c1-9d53-457b-b5d3-8388ac268352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\it\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in d:\\it\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in d:\\it\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\it\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in d:\\it\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\it\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1.tar.gz (57.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [47 lines of output]\n",
      "  + meson setup C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-install-yn_fv6ad\\scipy_e2a8e4ec96ba410892814011192d7fe9 C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-install-yn_fv6ad\\scipy_e2a8e4ec96ba410892814011192d7fe9\\.mesonpy-bla_6ipr -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-install-yn_fv6ad\\scipy_e2a8e4ec96ba410892814011192d7fe9\\.mesonpy-bla_6ipr\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.9.1\n",
      "  Source dir: C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-install-yn_fv6ad\\scipy_e2a8e4ec96ba410892814011192d7fe9\n",
      "  Build dir: C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-install-yn_fv6ad\\scipy_e2a8e4ec96ba410892814011192d7fe9\\.mesonpy-bla_6ipr\n",
      "  Build type: native build\n",
      "  Activating VS 17.14.16\n",
      "  Project name: scipy\n",
      "  Project version: 1.13.1\n",
      "  C compiler for the host machine: cl (msvc 19.44.35217 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.44.35217 for x64\")\n",
      "  C linker for the host machine: link link 14.44.35217.0\n",
      "  C++ compiler for the host machine: cl (msvc 19.44.35217 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.44.35217 for x64\")\n",
      "  C++ linker for the host machine: link link 14.44.35217.0\n",
      "  Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "  Host machine cpu family: x86_64\n",
      "  Host machine cpu: x86_64\n",
      "  Program python found: YES (D:\\IT\\python.exe)\n",
      "  Run-time dependency python found: YES 3.13\n",
      "  Program cython found: YES (C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-build-env-8b40o9yb\\overlay\\Scripts\\cython.EXE)\n",
      "  Compiler for C supports arguments -Wno-unused-but-set-variable: NO\n",
      "  Compiler for C supports arguments -Wno-unused-function: NO\n",
      "  Compiler for C supports arguments -Wno-conversion: NO\n",
      "  Compiler for C supports arguments -Wno-misleading-indentation: NO\n",
      "  Library m found: NO\n",
      "  \n",
      "  ..\\meson.build:78:0: ERROR: Unknown compiler(s): [['ifort'], ['gfortran'], ['flang-new'], ['flang'], ['pgfortran'], ['g95']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `ifort --help` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `ifort --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `ifort -V` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gfortran --help` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gfortran --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gfortran -V` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `flang-new --help` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `flang-new --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `flang-new -V` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `flang --help` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `flang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `flang -V` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgfortran --help` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgfortran --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgfortran -V` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `g95 --help` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `g95 --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `g95 -V` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\uula2\\AppData\\Local\\Temp\\pip-install-yn_fv6ad\\scipy_e2a8e4ec96ba410892814011192d7fe9\\.mesonpy-bla_6ipr\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\it\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\it\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\it\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\it\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\it\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: seaborn in d:\\it\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in d:\\it\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\it\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in d:\\it\\lib\\site-packages (from seaborn) (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\it\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\it\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\it\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\it\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db09f6c5-e792-4dd7-a733-a33743a7a771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Word2Vec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Машинне навчання\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Базові бібліотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP та обробка тексту\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Машинне навчання\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Моделі\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Ініціалізація nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb8114-fae1-4bfe-a800-e9fb95ac95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завантаження даних\n",
    "train_df = pd.read_csv(\"Corona_NLP_train.csv\")\n",
    "test_df = pd.read_csv(\"Corona_NLP_test.csv\")\n",
    "\n",
    "# Перевірка колонок\n",
    "print(train_df.columns)\n",
    "train_df.head()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Приведення до нижнього регістру\n",
    "    text = text.lower()\n",
    "    # Видалення URL, @username, #hashtags, цифр та спецсимволів\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Токенізація\n",
    "    tokens = word_tokenize(text)\n",
    "    # Видалення стоп-слів\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Обробка тексту\n",
    "train_df['tokens'] = train_df['OriginalTweet'].apply(preprocess_text)\n",
    "test_df['tokens'] = test_df['OriginalTweet'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bfd55-3812-4fd9-94fc-d8ec74a56801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Навчання Word2Vec\n",
    "w2v_model = Word2Vec(sentences=train_df['tokens'], vector_size=100, window=5, min_count=2, workers=4, sg=1)\n",
    "\n",
    "# Функція для отримання вектора твітів\n",
    "def tweet_vector(tokens, model, vector_size):\n",
    "    vec = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Генерація векторів для тренувальної та тестової вибірки\n",
    "vector_size = 100\n",
    "X_train = np.array([tweet_vector(x, w2v_model, vector_size) for x in train_df['tokens']])\n",
    "X_test = np.array([tweet_vector(x, w2v_model, vector_size) for x in test_df['tokens']])\n",
    "\n",
    "# Мітки\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['Sentiment'])\n",
    "y_test = le.transform(test_df['Sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e183c-6fb2-4b3b-859b-d077a3864112",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1-score': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "# Таблиця результатів\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c0cdf-065d-47ab-9ff4-4f04a4ca1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = [50, 100, 200]\n",
    "pca_results = {}\n",
    "\n",
    "for n in pca_components:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_pca, y_train)\n",
    "        y_pred = model.predict(X_test_pca)\n",
    "        \n",
    "        pca_results[(name, n)] = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "            'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "            'F1-score': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "\n",
    "# Таблиця результатів PCA\n",
    "pca_results_df = pd.DataFrame(pca_results).T\n",
    "pca_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462ff1a-bc2f-4d4e-a227-d05af4a123a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наприклад, виберемо Logistic Regression без PCA\n",
    "best_model = LogisticRegression(max_iter=500)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
