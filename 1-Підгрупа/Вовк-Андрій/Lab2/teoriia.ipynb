{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1afc911",
   "metadata": {},
   "source": [
    "# Теоретичний матеріал: Логістична регресія\n",
    "\n",
    "## Зміст\n",
    "1. [Вступ до логістичної регресії](#1)\n",
    "2. [Математичні основи](#2)\n",
    "3. [Функція втрат](#3)\n",
    "4. [Методи оптимізації](#4)\n",
    "5. [Регуляризація](#5)\n",
    "6. [Метрики оцінки якості](#6)\n",
    "7. [Практичні рекомендації](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3ba88",
   "metadata": {},
   "source": [
    "## 1. Вступ до логістичної регресії <a id=\"1\"></a>\n",
    "\n",
    "### Що таке логістична регресія?\n",
    "\n",
    "**Логістична регресія** — це статистичний метод для розв'язання задач **бінарної класифікації**. На відміну від лінійної регресії, яка передбачає безперервні значення, логістична регресія передбачає ймовірність належності до одного з двох класів.\n",
    "\n",
    "### Основні характеристики:\n",
    "- **Вихідні значення**: від 0 до 1 (ймовірності)\n",
    "- **Функція активації**: сигмоїдна функція\n",
    "- **Тип задачі**: класифікація\n",
    "- **Припущення**: лінійна залежність між ознаками та логарифмом відношення шансів\n",
    "\n",
    "### Переваги:\n",
    "✅ Простота інтерпретації  \n",
    "✅ Не потребує налаштування гіперпараметрів  \n",
    "✅ Не схильна до перенавчання при малій кількості ознак  \n",
    "✅ Швидке навчання  \n",
    "✅ Не потребує масштабування ознак (але краще робити)  \n",
    "\n",
    "### Недоліки:\n",
    "❌ Припускає лінійну залежність між ознаками та логітом  \n",
    "❌ Може бути чутливою до викидів  \n",
    "❌ Потребує великих обсягів даних для стабільних результатів  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64adc53",
   "metadata": {},
   "source": [
    "## 2. Математичні основи <a id=\"2\"></a>\n",
    "\n",
    "### 2.1 Лінійна комбінація\n",
    "\n",
    "Спочатку обчислюється лінійна комбінація ознак:\n",
    "\n",
    "$$z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n = \\sum_{i=0}^{n} w_i x_i$$\n",
    "\n",
    "де:\n",
    "- $w_i$ — ваги (параметри моделі)\n",
    "- $x_i$ — значення ознак\n",
    "- $w_0$ — bias term (зсув)\n",
    "\n",
    "### 2.2 Сигмоїдна функція\n",
    "\n",
    "Сигмоїдна функція перетворює будь-яке дійсне число в інтервал (0, 1):\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Властивості сигмоїда:**\n",
    "- При $z \\to +\\infty$: $\\sigma(z) \\to 1$\n",
    "- При $z \\to -\\infty$: $\\sigma(z) \\to 0$\n",
    "- При $z = 0$: $\\sigma(z) = 0.5$\n",
    "- Похідна: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "### 2.3 Ймовірнісна інтерпретація\n",
    "\n",
    "Результат сигмоїда інтерпретується як ймовірність:\n",
    "\n",
    "$$P(y=1|x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}$$\n",
    "\n",
    "$$P(y=0|x) = 1 - P(y=1|x) = \\frac{e^{-w^T x}}{1 + e^{-w^T x}}$$\n",
    "\n",
    "### 2.4 Логіт (Log-odds)\n",
    "\n",
    "Логістична регресія моделює логарифм відношення шансів:\n",
    "\n",
    "$$\\ln\\left(\\frac{P(y=1|x)}{P(y=0|x)}\\right) = \\ln\\left(\\frac{P(y=1|x)}{1-P(y=1|x)}\\right) = w^T x$$\n",
    "\n",
    "Це пояснює, чому метод називається \"логістична регресія\" — він лінійно моделює логіт."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f1dc6",
   "metadata": {},
   "source": [
    "## 3. Функція втрат (Loss Function) <a id=\"3\"></a>\n",
    "\n",
    "### 3.1 Binary Cross-Entropy (Log-Loss)\n",
    "\n",
    "Для бінарної класифікації використовується **логарифмічна функція втрат**:\n",
    "\n",
    "$$L(w) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\ln(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\ln(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "\n",
    "де:\n",
    "- $m$ — кількість зразків\n",
    "- $y^{(i)}$ — справжнє значення класу (0 або 1)\n",
    "- $\\hat{y}^{(i)} = \\sigma(w^T x^{(i)})$ — передбачена ймовірність\n",
    "\n",
    "### 3.2 Інтуїція функції втрат\n",
    "\n",
    "**Для $y = 1$ (позитивний клас):**\n",
    "- Втрата = $-\\ln(\\hat{y})$\n",
    "- Якщо $\\hat{y} \\to 1$: втрата $\\to 0$ (добре)\n",
    "- Якщо $\\hat{y} \\to 0$: втрата $\\to +\\infty$ (погано)\n",
    "\n",
    "**Для $y = 0$ (негативний клас):**\n",
    "- Втрата = $-\\ln(1 - \\hat{y})$\n",
    "- Якщо $\\hat{y} \\to 0$: втрата $\\to 0$ (добре)\n",
    "- Якщо $\\hat{y} \\to 1$: втрата $\\to +\\infty$ (погано)\n",
    "\n",
    "### 3.3 Чому не середньоквадратична похибка?\n",
    "\n",
    "MSE для логістичної регресії:\n",
    "$$L_{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\sigma(w^T x^{(i)}))^2$$\n",
    "\n",
    "**Проблеми MSE:**\n",
    "- ❌ Неопукла функція (локальні мінімуми)\n",
    "- ❌ Повільна збіжність\n",
    "- ❌ Проблеми з градієнтами при насиченні сигмоїда\n",
    "\n",
    "**Переваги Cross-Entropy:**\n",
    "- ✅ Опукла функція\n",
    "- ✅ Швидка збіжність\n",
    "- ✅ Максимізує правдоподібність"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23458256",
   "metadata": {},
   "source": [
    "## 4. Методи оптимізації <a id=\"4\"></a>\n",
    "\n",
    "### 4.1 Градієнтний спуск\n",
    "\n",
    "Для мінімізації функції втрат використовується градієнтний спуск:\n",
    "\n",
    "$$w := w - \\alpha \\nabla_w L(w)$$\n",
    "\n",
    "де $\\alpha$ — швидкість навчання (learning rate).\n",
    "\n",
    "### 4.2 Обчислення градієнтів\n",
    "\n",
    "Градієнт функції втрат відносно ваг:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "У векторній формі:\n",
    "$$\\nabla_w L = \\frac{1}{m} X^T (\\hat{y} - y)$$\n",
    "\n",
    "### 4.3 Варіанти градієнтного спуску\n",
    "\n",
    "#### 4.3.1 Batch Gradient Descent (BGD)\n",
    "Використовує весь датасет для обчислення градієнта:\n",
    "```\n",
    "для кожної епохи:\n",
    "    обчислити градієнт на всьому датасеті\n",
    "    оновити ваги\n",
    "```\n",
    "\n",
    "**Переваги:** Стабільна збіжність, гладкі криві втрат  \n",
    "**Недоліки:** Повільний на великих датасетах, може застрягти в локальних мінімумах\n",
    "\n",
    "#### 4.3.2 Stochastic Gradient Descent (SGD)\n",
    "Використовує один зразок для обчислення градієнта:\n",
    "```\n",
    "для кожної епохи:\n",
    "    перемішати датасет\n",
    "    для кожного зразка:\n",
    "        обчислити градієнт на одному зразку\n",
    "        оновити ваги\n",
    "```\n",
    "\n",
    "**Переваги:** Швидкий, може виходити з локальних мінімумів, онлайн-навчання  \n",
    "**Недоліки:** Шумні градієнти, нестабільна збіжність\n",
    "\n",
    "#### 4.3.3 Mini-batch Gradient Descent\n",
    "Компроміс між BGD та SGD — використовує невеликі групи зразків:\n",
    "```\n",
    "для кожної епохи:\n",
    "    перемішати датасет\n",
    "    розділити на batch'і розміру batch_size\n",
    "    для кожного batch'а:\n",
    "        обчислити градієнт на batch'і\n",
    "        оновити ваги\n",
    "```\n",
    "\n",
    "**Переваги:** Збалансований, ефективне використання векторизації, стабільніший за SGD  \n",
    "**Недоліки:** Потребує налаштування розміру batch'а\n",
    "\n",
    "### 4.4 Вибір розміру batch'а\n",
    "\n",
    "- **Малий batch (8-32):** Більше шуму, швидше навчання, менше пам'яті\n",
    "- **Середній batch (64-256):** Золотий стандарт для більшості задач\n",
    "- **Великий batch (512+):** Стабільніше, але може призвести до поганої генералізації"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2510b3",
   "metadata": {},
   "source": [
    "## 5. Регуляризація <a id=\"5\"></a>\n",
    "\n",
    "### 5.1 Проблема перенавчання\n",
    "\n",
    "**Перенавчання (Overfitting)** — коли модель занадто добре \"запам'ятовує\" тренувальні дані і погано узагальнює на нові дані.\n",
    "\n",
    "**Ознаки перенавчання:**\n",
    "- Низька втрата на тренуванні, висока на валідації\n",
    "- Великі значення ваг\n",
    "- Складна модель відносно кількості даних\n",
    "\n",
    "### 5.2 L2 регуляризація (Ridge)\n",
    "\n",
    "Додає штраф за великі ваги до функції втрат:\n",
    "\n",
    "$$L_{Ridge}(w) = L_{original}(w) + \\frac{\\alpha}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "**Ефект:** Зменшує всі ваги рівномірно, робить їх більш \"розумними\"\n",
    "\n",
    "**Градієнт з L2:**\n",
    "$$\\frac{\\partial L_{Ridge}}{\\partial w_j} = \\frac{\\partial L_{original}}{\\partial w_j} + \\frac{\\alpha}{m} w_j$$\n",
    "\n",
    "### 5.3 L1 регуляризація (Lasso)\n",
    "\n",
    "Додає штраф за абсолютні значення ваг:\n",
    "\n",
    "$$L_{Lasso}(w) = L_{original}(w) + \\frac{\\alpha}{m} \\sum_{j=1}^{n} |w_j|$$\n",
    "\n",
    "**Ефект:** Може робити деякі ваги рівними нулю (автоматичний відбір ознак)\n",
    "\n",
    "**Градієнт з L1:**\n",
    "$$\\frac{\\partial L_{Lasso}}{\\partial w_j} = \\frac{\\partial L_{original}}{\\partial w_j} + \\frac{\\alpha}{m} \\text{sign}(w_j)$$\n",
    "\n",
    "### 5.4 Порівняння L1 vs L2\n",
    "\n",
    "| Аспект | L1 (Lasso) | L2 (Ridge) |\n",
    "|--------|------------|------------|\n",
    "| **Форма штрафу** | $\\|w\\|_1 = \\sum |w_i|$ | $\\|w\\|_2^2 = \\sum w_i^2$ |\n",
    "| **Геометрія** | Ромб | Коло |\n",
    "| **Розрідженість** | Створює розріджені рішення | Зменшує всі ваги |\n",
    "| **Відбір ознак** | Автоматичний | Ні |\n",
    "| **Стійкість** | Менш стійкий | Більш стійкий |\n",
    "| **Використання** | Коли багато шумних ознак | Коли всі ознаки корисні |\n",
    "\n",
    "### 5.5 Elastic Net\n",
    "\n",
    "Комбінує L1 та L2:\n",
    "$$L_{ElasticNet}(w) = L_{original}(w) + \\alpha \\left[ \\rho \\|w\\|_1 + \\frac{1-\\rho}{2} \\|w\\|_2^2 \\right]$$\n",
    "\n",
    "де $\\rho \\in [0, 1]$ контролює баланс між L1 та L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13442b5d",
   "metadata": {},
   "source": [
    "## 6. Метрики оцінки якості <a id=\"6\"></a>\n",
    "\n",
    "### 6.1 Confusion Matrix (Матриця плутанини)\n",
    "\n",
    "```\n",
    "                  Передбачене\n",
    "                Neg    Pos\n",
    "Справжнє  Neg   TN     FP\n",
    "          Pos   FN     TP\n",
    "```\n",
    "\n",
    "де:\n",
    "- **TP (True Positive)** — правильно передбачені позитивні\n",
    "- **TN (True Negative)** — правильно передбачені негативні\n",
    "- **FP (False Positive)** — помилково передбачені як позитивні (Type I error)\n",
    "- **FN (False Negative)** — пропущені позитивні (Type II error)\n",
    "\n",
    "### 6.2 Основні метрики\n",
    "\n",
    "#### 6.2.1 Accuracy (Точність)\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Інтерпретація:** Частка правильних передбачень  \n",
    "**Проблема:** Може вводити в оману при незбалансованих класах\n",
    "\n",
    "#### 6.2.2 Precision (Точність позитивного класу)\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Інтерпретація:** З усіх передбачених позитивних, скільки справді позитивні?  \n",
    "**Коли важливо:** Коли False Positive дорого коштують\n",
    "\n",
    "#### 6.2.3 Recall (Sensitivity, True Positive Rate)\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Інтерпретація:** З усіх справжніх позитивних, скільки ми знайшли?  \n",
    "**Коли важливо:** Коли False Negative дорого коштують\n",
    "\n",
    "#### 6.2.4 Specificity (True Negative Rate)\n",
    "$$Specificity = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "**Інтерпретація:** З усіх справжніх негативних, скільки ми правильно класифікували?\n",
    "\n",
    "#### 6.2.5 F1-Score\n",
    "$$F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "**Інтерпретація:** Гармонічне середнє Precision та Recall  \n",
    "**Переваги:** Збалансована метрика, особливо для незбалансованих класів\n",
    "\n",
    "### 6.3 ROC та AUC\n",
    "\n",
    "#### ROC Curve (Receiver Operating Characteristic)\n",
    "Графік залежності True Positive Rate від False Positive Rate при різних порогах:\n",
    "- **TPR** = Recall = $\\frac{TP}{TP + FN}$\n",
    "- **FPR** = $\\frac{FP}{FP + TN}$\n",
    "\n",
    "#### AUC (Area Under Curve)\n",
    "Площа під ROC кривою:\n",
    "- **AUC = 1.0:** Ідеальна модель\n",
    "- **AUC = 0.5:** Випадкове вгадування\n",
    "- **AUC < 0.5:** Гірше за випадковість (можна інвертувати)\n",
    "\n",
    "### 6.4 Вибір порогу класифікації\n",
    "\n",
    "За замовчуванням поріг = 0.5, але можна оптимізувати:\n",
    "\n",
    "1. **Максимізація F1-score**\n",
    "2. **Рівні Precision та Recall**\n",
    "3. **Максимізація Youden's Index** (TPR - FPR)\n",
    "4. **Бізнес-метрики** (мінімізація втрат)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9855d3",
   "metadata": {},
   "source": [
    "## 7. Практичні рекомендації <a id=\"7\"></a>\n",
    "\n",
    "### 7.1 Підготовка даних\n",
    "\n",
    "#### 7.1.1 Масштабування ознак\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "**Чому важливо:** Різні масштаби ознак можуть призвести до домінування одних ознак над іншими\n",
    "\n",
    "#### 7.1.2 Обробка категоріальних змінних\n",
    "```python\n",
    "X_encoded = pd.get_dummies(X_categorical, drop_first=True)\n",
    "```\n",
    "\n",
    "#### 7.1.3 Розділення даних\n",
    "```python\n",
    "# Типове розділення\n",
    "Train: 60-80%\n",
    "Validation: 10-20%\n",
    "Test: 10-20%\n",
    "```\n",
    "\n",
    "### 7.2 Налаштування гіперпараметрів\n",
    "\n",
    "#### 7.2.1 Learning Rate (α)\n",
    "- **Занадто великий:** Модель не збігається, \"стрибає\" навколо мінімуму\n",
    "- **Занадто малий:** Повільна збіжність\n",
    "- **Рекомендація:** Починати з 0.01, адаптивно зменшувати\n",
    "\n",
    "#### 7.2.2 Розмір batch'а\n",
    "- **Малий (8-32):** Більше шуму, краща генералізація\n",
    "- **Середній (64-256):** Баланс між стабільністю та швидкістю\n",
    "- **Великий (512+):** Стабільна збіжність, але може призвести до перенавчання\n",
    "\n",
    "#### 7.2.3 Регуляризаційний параметр (λ)\n",
    "- **λ = 0:** Без регуляризації\n",
    "- **Малий λ (0.001-0.01):** Легка регуляризація\n",
    "- **Середній λ (0.1-1):** Помірна регуляризація\n",
    "- **Великий λ (10+):** Сильна регуляризація\n",
    "\n",
    "### 7.3 Діагностика моделі\n",
    "\n",
    "#### 7.3.1 Підгонка vs Перенавчання vs Недонавчання\n",
    "\n",
    "| Стан | Train Loss | Val Loss | Дії |\n",
    "|------|------------|----------|----- |\n",
    "| **Недонавчання** | Високий | Високий | ↑ складність, ↓ регуляризація |\n",
    "| **Добра підгонка** | Низький | Низький | Готово! |\n",
    "| **Перенавчання** | Низький | Високий | ↑ регуляризація, ↑ дані |\n",
    "\n",
    "#### 7.3.2 Криві навчання\n",
    "```python\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "**Хорошні ознаки:**\n",
    "- ✅ Обидві криві спадають\n",
    "- ✅ Невеликий розрив між ними\n",
    "- ✅ Стабілізація в кінці\n",
    "\n",
    "**Погані ознаки:**\n",
    "- ❌ Validation loss зростає\n",
    "- ❌ Великий розрив між кривими\n",
    "- ❌ Коливання без стабілізації\n",
    "\n",
    "### 7.4 Покращення якості моделі\n",
    "\n",
    "#### 7.4.1 Feature Engineering\n",
    "1. **Створення нових ознак:** поліноміальні, взаємодії\n",
    "2. **Трансформації:** log, sqrt, Box-Cox\n",
    "3. **Біннінг:** дискретизація безперервних змінних\n",
    "4. **Domain knowledge:** спеціалізовані ознаки для задачі\n",
    "\n",
    "#### 7.4.2 Обробка дисбалансу класів\n",
    "1. **Oversampling:** SMOTE, ADASYN\n",
    "2. **Undersampling:** випадкове, Tomek links\n",
    "3. **Class weights:** штрафувати за помилки в меншому класі\n",
    "4. **Threshold tuning:** оптимізація порогу класифікації\n",
    "\n",
    "#### 7.4.3 Ensemble методи\n",
    "1. **Voting:** комбінування кількох моделей\n",
    "2. **Bagging:** тренування на різних підвибірках\n",
    "3. **Stacking:** мета-модель над базовими моделями\n",
    "\n",
    "### 7.5 Коли НЕ використовувати логістичну регресію\n",
    "\n",
    "❌ **Нелінійні залежності:** Використовуйте SVM з RBF, Random Forest, Neural Networks  \n",
    "❌ **Малі дані (<100 зразків):** Ризик перенавчання  \n",
    "❌ **Багато шумних ознак:** Краще використати методи з автоматичним відбором ознак  \n",
    "❌ **Мультикласова класифікація:** Хоча можна (One-vs-Rest), спеціалізовані методи часто кращі  \n",
    "❌ **Вимоги до інтерпретації складних взаємодій:** Decision Trees може бути кращим вибором  \n",
    "\n",
    "### 7.6 Чек-лист для успішної імплементації\n",
    "\n",
    "**Перед навчанням:**\n",
    "- [ ] Дані очищені та підготовані\n",
    "- [ ] Ознаки масштабовані\n",
    "- [ ] Категоріальні змінні закодовані\n",
    "- [ ] Дані розділені на train/val/test\n",
    "- [ ] Дисбаланс класів врахований\n",
    "\n",
    "**Під час навчання:**\n",
    "- [ ] Моніторинг кривих навчання\n",
    "- [ ] Early stopping для запобігання перенавчанню\n",
    "- [ ] Валідація на відкладеній вибірці\n",
    "- [ ] Логування експериментів\n",
    "\n",
    "**Після навчання:**\n",
    "- [ ] Оцінка на тестовій вибірці\n",
    "- [ ] Аналіз помилок\n",
    "- [ ] Перевірка стабільності результатів\n",
    "- [ ] Документування результатів"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
