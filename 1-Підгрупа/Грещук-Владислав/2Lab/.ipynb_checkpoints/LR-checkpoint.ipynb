{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37996ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмір датасету: (10000, 16)\n",
      "\n",
      "Перші рядки:\n",
      "   ID  Age  Gender  Country  Coffee_Intake  Caffeine_mg  Sleep_Hours  \\\n",
      "0   1   40    Male  Germany            3.5        328.1          7.5   \n",
      "1   2   33    Male  Germany            1.0         94.1          6.2   \n",
      "2   3   42    Male   Brazil            5.3        503.7          5.9   \n",
      "3   4   53    Male  Germany            2.6        249.2          7.3   \n",
      "4   5   32  Female    Spain            3.1        298.0          5.3   \n",
      "\n",
      "  Sleep_Quality   BMI  Heart_Rate Stress_Level  Physical_Activity_Hours  \\\n",
      "0          Good  24.9          78          Low                     14.5   \n",
      "1          Good  20.0          67          Low                     11.0   \n",
      "2          Fair  22.7          59       Medium                     11.2   \n",
      "3          Good  24.7          71          Low                      6.6   \n",
      "4          Fair  24.1          76       Medium                      8.5   \n",
      "\n",
      "  Health_Issues Occupation  Smoking  Alcohol_Consumption  \n",
      "0           NaN      Other        0                    0  \n",
      "1           NaN    Service        0                    0  \n",
      "2          Mild     Office        0                    0  \n",
      "3          Mild      Other        0                    0  \n",
      "4          Mild    Student        0                    1  \n",
      "\n",
      "Інформація про датасет:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ID                       10000 non-null  int64  \n",
      " 1   Age                      10000 non-null  int64  \n",
      " 2   Gender                   10000 non-null  object \n",
      " 3   Country                  10000 non-null  object \n",
      " 4   Coffee_Intake            10000 non-null  float64\n",
      " 5   Caffeine_mg              10000 non-null  float64\n",
      " 6   Sleep_Hours              10000 non-null  float64\n",
      " 7   Sleep_Quality            10000 non-null  object \n",
      " 8   BMI                      10000 non-null  float64\n",
      " 9   Heart_Rate               10000 non-null  int64  \n",
      " 10  Stress_Level             10000 non-null  object \n",
      " 11  Physical_Activity_Hours  10000 non-null  float64\n",
      " 12  Health_Issues            4059 non-null   object \n",
      " 13  Occupation               10000 non-null  object \n",
      " 14  Smoking                  10000 non-null  int64  \n",
      " 15  Alcohol_Consumption      10000 non-null  int64  \n",
      "dtypes: float64(5), int64(5), object(6)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "\n",
      "Статистика:\n",
      "                ID           Age  Coffee_Intake   Caffeine_mg   Sleep_Hours  \\\n",
      "count  10000.00000  10000.000000   10000.000000  10000.000000  10000.000000   \n",
      "mean    5000.50000     34.949100       2.509230    238.411010      6.636220   \n",
      "std     2886.89568     11.160939       1.450248    137.748815      1.222055   \n",
      "min        1.00000     18.000000       0.000000      0.000000      3.000000   \n",
      "25%     2500.75000     26.000000       1.500000    138.750000      5.800000   \n",
      "50%     5000.50000     34.000000       2.500000    235.400000      6.600000   \n",
      "75%     7500.25000     43.000000       3.500000    332.025000      7.500000   \n",
      "max    10000.00000     80.000000       8.200000    780.300000     10.000000   \n",
      "\n",
      "                BMI    Heart_Rate  Physical_Activity_Hours      Smoking  \\\n",
      "count  10000.000000  10000.000000              10000.00000  10000.00000   \n",
      "mean      23.986860     70.617800                  7.48704      0.20040   \n",
      "std        3.906411      9.822951                  4.31518      0.40032   \n",
      "min       15.000000     50.000000                  0.00000      0.00000   \n",
      "25%       21.300000     64.000000                  3.70000      0.00000   \n",
      "50%       24.000000     71.000000                  7.50000      0.00000   \n",
      "75%       26.600000     77.000000                 11.20000      0.00000   \n",
      "max       38.200000    109.000000                 15.00000      1.00000   \n",
      "\n",
      "       Alcohol_Consumption  \n",
      "count         10000.000000  \n",
      "mean              0.300700  \n",
      "std               0.458585  \n",
      "min               0.000000  \n",
      "25%               0.000000  \n",
      "50%               0.000000  \n",
      "75%               1.000000  \n",
      "max               1.000000  \n",
      "Пропущені значення:\n",
      "ID                            0\n",
      "Age                           0\n",
      "Gender                        0\n",
      "Country                       0\n",
      "Coffee_Intake                 0\n",
      "Caffeine_mg                   0\n",
      "Sleep_Hours                   0\n",
      "Sleep_Quality                 0\n",
      "BMI                           0\n",
      "Heart_Rate                    0\n",
      "Stress_Level                  0\n",
      "Physical_Activity_Hours       0\n",
      "Health_Issues              5941\n",
      "Occupation                    0\n",
      "Smoking                       0\n",
      "Alcohol_Consumption           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Test Results'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Test Results'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.isnull().sum())\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ## 3. Створення цільової змінної та видалення непотрібних стовпців\u001b[39;00m\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Створення бінарної цільової змінної: 1 - Normal, 0 - Abnormal/Inconclusive\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mTest_Result_Normal\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTest Results\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[33m'\u001b[39m\u001b[33mNormal\u001b[39m\u001b[33m'\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mРозподіл цільової змінної:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mTest_Result_Normal\u001b[39m\u001b[33m'\u001b[39m].value_counts())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Test Results'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Бінарна класифікація медичних тестів з використанням логістичної регресії\n",
    "# \n",
    "# **Мета:** Передбачити результат медичного тесту (Normal vs Abnormal/Inconclusive) на основі медично-демографічних факторів.\n",
    "# \n",
    "# **Модель:** Логістична регресія з реалізацією SGD та Mini-batch градієнтного спуску.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Імпорт бібліотек\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Налаштування візуалізації\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Завантаження та підготовка даних\n",
    "\n",
    "# %%\n",
    "# Завантаження даних\n",
    "df = pd.read_csv('synthetic_coffee_health_10000.csv')\n",
    "\n",
    "print(\"Розмір датасету:\", df.shape)\n",
    "print(\"\\nПерші рядки:\")\n",
    "print(df.head())\n",
    "print(\"\\nІнформація про датасет:\")\n",
    "print(df.info())\n",
    "print(\"\\nСтатистика:\")\n",
    "print(df.describe())\n",
    "\n",
    "# %%\n",
    "# Перевірка пропущених значень\n",
    "print(\"Пропущені значення:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Створення цільової змінної та видалення непотрібних стовпців\n",
    "\n",
    "# %%\n",
    "# Створення бінарної цільової змінної: 1 - Normal, 0 - Abnormal/Inconclusive\n",
    "df['Test_Result_Normal'] = (df['Test Results'] == 'Normal').astype(int)\n",
    "\n",
    "print(\"Розподіл цільової змінної:\")\n",
    "print(df['Test_Result_Normal'].value_counts())\n",
    "print(\"\\nВідсоткове співвідношення:\")\n",
    "print(df['Test_Result_Normal'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Видалення непотрібних стовпців\n",
    "columns_to_drop = ['Name', 'Date of Admission', 'Doctor', 'Hospital', \n",
    "                   'Room Number', 'Discharge Date', 'Test Results']\n",
    "df_clean = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "print(\"\\nСтовпці після очищення:\")\n",
    "print(df_clean.columns.tolist())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Аналіз даних\n",
    "# \n",
    "# ### 4.1 Візуалізація розподілу цільової змінної\n",
    "\n",
    "# %%\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Графік 1: Стовпчаста діаграма\n",
    "df['Test_Result_Normal'].value_counts().plot(kind='bar', ax=axes[0], color=['#e74c3c', '#2ecc71'])\n",
    "axes[0].set_title('Розподіл результатів тестів', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Результат (0 = Abnormal, 1 = Normal)', fontsize=12)\n",
    "axes[0].set_ylabel('Кількість', fontsize=12)\n",
    "axes[0].set_xticklabels(['Abnormal/Inconclusive', 'Normal'], rotation=0)\n",
    "\n",
    "# Графік 2: Кругова діаграма\n",
    "df['Test_Result_Normal'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%',\n",
    "                                              colors=['#e74c3c', '#2ecc71'],\n",
    "                                              labels=['Abnormal/Inconclusive', 'Normal'])\n",
    "axes[1].set_title('Відсоткове співвідношення', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.2 Аналіз числових ознак\n",
    "\n",
    "# %%\n",
    "# Візуалізація розподілу числових ознак\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col != 'Test_Result_Normal']\n",
    "\n",
    "if numeric_cols:\n",
    "    fig, axes = plt.subplots(1, len(numeric_cols), figsize=(6*len(numeric_cols), 5))\n",
    "    if len(numeric_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        axes[idx].hist(df_clean[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Розподіл {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col, fontsize=10)\n",
    "        axes[idx].set_ylabel('Частота', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.3 Кореляційна матриця\n",
    "\n",
    "# %%\n",
    "# One-hot encoding для побудови кореляційної матриці\n",
    "df_encoded = pd.get_dummies(df_clean, drop_first=True)\n",
    "\n",
    "# Кореляційна матриця\n",
    "correlation_matrix = df_encoded.corr()\n",
    "\n",
    "# Візуалізація\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Кореляційна матриця ознак', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Топ-10 кореляцій з цільовою змінною\n",
    "target_corr = correlation_matrix['Test_Result_Normal'].sort_values(ascending=False)\n",
    "print(\"\\nТоп-10 ознак за кореляцією з Test_Result_Normal:\")\n",
    "print(target_corr.head(11))  # 11, бо перша - сама цільова змінна\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Підготовка даних для моделі\n",
    "# \n",
    "# ### 5.1 One-hot encoding категоріальних ознак\n",
    "\n",
    "# %%\n",
    "# Відокремлення цільової змінної\n",
    "y = df_clean['Test_Result_Normal'].values\n",
    "X = df_clean.drop('Test_Result_Normal', axis=1)\n",
    "\n",
    "# One-hot encoding\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Категоріальні стовпці для кодування: {categorical_cols}\")\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "print(f\"\\nКількість ознак після one-hot encoding: {X_encoded.shape[1]}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.2 Нормалізація числових ознак\n",
    "\n",
    "# %%\n",
    "# Нормалізація (стандартизація)\n",
    "def normalize_features(X):\n",
    "    \"\"\"Z-score нормалізація\"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    # Уникненняділення на нуль\n",
    "    std[std == 0] = 1\n",
    "    return (X - mean) / std, mean, std\n",
    "\n",
    "X_normalized, X_mean, X_std = normalize_features(X_encoded.values)\n",
    "print(\"Дані нормалізовано\")\n",
    "print(f\"Форма X: {X_normalized.shape}\")\n",
    "print(f\"Форма y: {y.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.3 Розділення на тренувальний, валідаційний та тестовий набори\n",
    "\n",
    "# %%\n",
    "# Спочатку розділимо на train (60%) та temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_normalized, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Потім temp розділимо на validation (20%) та test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Розмір тренувального набору: {X_train.shape}\")\n",
    "print(f\"Розмір валідаційного набору: {X_val.shape}\")\n",
    "print(f\"Розмір тестового набору: {X_test.shape}\")\n",
    "\n",
    "# Розподіл класів\n",
    "print(f\"\\nТренувальний набір - Normal: {np.sum(y_train)} ({np.mean(y_train)*100:.1f}%)\")\n",
    "print(f\"Валідаційний набір - Normal: {np.sum(y_val)} ({np.mean(y_val)*100:.1f}%)\")\n",
    "print(f\"Тестовий набір - Normal: {np.sum(y_test)} ({np.mean(y_test)*100:.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Реалізація логістичної регресії\n",
    "# \n",
    "# ### 6.1 Основні функції\n",
    "\n",
    "# %%\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Сигмоїдна функція активації\n",
    "    σ(z) = 1 / (1 + e^(-z))\n",
    "    \"\"\"\n",
    "    # Обмеження для уникнення overflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(X, y, weights, lambda_l1=0, lambda_l2=0):\n",
    "    \"\"\"\n",
    "    Обчислення Log Loss (Binary Cross-Entropy) з регуляризацією\n",
    "    \n",
    "    L = -1/m * Σ[y*log(ŷ) + (1-y)*log(1-ŷ)] + регуляризація\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, weights))\n",
    "    \n",
    "    # Уникнення log(0)\n",
    "    epsilon = 1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Основна функція втрат\n",
    "    loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    \n",
    "    # Додавання регуляризації (не застосовуємо до intercept - перша вага)\n",
    "    if lambda_l2 > 0:\n",
    "        loss += (lambda_l2 / (2 * m)) * np.sum(weights[1:]**2)\n",
    "    if lambda_l1 > 0:\n",
    "        loss += (lambda_l1 / m) * np.sum(np.abs(weights[1:]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, y, weights, lambda_l1=0, lambda_l2=0):\n",
    "    \"\"\"\n",
    "    Обчислення градієнта функції втрат\n",
    "    \n",
    "    ∂L/∂w = 1/m * X^T * (ŷ - y) + регуляризація\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(np.dot(X, weights))\n",
    "    gradient = np.dot(X.T, (predictions - y)) / m\n",
    "    \n",
    "    # Додавання регуляризації (не застосовуємо до intercept)\n",
    "    if lambda_l2 > 0:\n",
    "        gradient[1:] += (lambda_l2 / m) * weights[1:]\n",
    "    if lambda_l1 > 0:\n",
    "        gradient[1:] += (lambda_l1 / m) * np.sign(weights[1:])\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "def initialize_weights(n_features):\n",
    "    \"\"\"Ініціалізація ваг малими випадковими значеннями\"\"\"\n",
    "    return np.random.randn(n_features) * 0.01\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6.2 Стохастичний градієнтний спуск (SGD)\n",
    "\n",
    "# %%\n",
    "def sgd(X, y, X_val, y_val, learning_rate=0.01, max_epochs=100, \n",
    "        lambda_l1=0, lambda_l2=0, patience=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Стохастичний градієнтний спуск\n",
    "    Оновлення ваг після кожного прикладу\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    weights = initialize_weights(n_features)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Перемішування даних на кожній епосі\n",
    "        indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Оновлення ваг для кожного прикладу\n",
    "        for i in range(len(X)):\n",
    "            X_i = X_shuffled[i:i+1]\n",
    "            y_i = y_shuffled[i:i+1]\n",
    "            \n",
    "            gradient = compute_gradient(X_i, y_i, weights, lambda_l1, lambda_l2)\n",
    "            weights -= learning_rate * gradient\n",
    "        \n",
    "        # Обчислення втрат після епохи\n",
    "        train_loss = compute_loss(X, y, weights, lambda_l1, lambda_l2)\n",
    "        val_loss = compute_loss(X_val, y_val, weights, lambda_l1, lambda_l2)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_weights = weights.copy()\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping на епосі {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Епоха {epoch+1}/{max_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nНайкраща модель на епосі {best_epoch} з Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_weights, train_losses, val_losses, best_epoch\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6.3 Mini-batch градієнтний спуск\n",
    "\n",
    "# %%\n",
    "def mini_batch_gd(X, y, X_val, y_val, learning_rate=0.01, batch_size=32, \n",
    "                  max_epochs=100, lambda_l1=0, lambda_l2=0, patience=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Mini-batch градієнтний спуск\n",
    "    Оновлення ваг після кожного батчу\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    weights = initialize_weights(n_features)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Перемішування даних\n",
    "        indices = np.random.permutation(len(X))\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Оновлення ваг для кожного батчу\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            gradient = compute_gradient(X_batch, y_batch, weights, lambda_l1, lambda_l2)\n",
    "            weights -= learning_rate * gradient\n",
    "        \n",
    "        # Обчислення втрат після епохи\n",
    "        train_loss = compute_loss(X, y, weights, lambda_l1, lambda_l2)\n",
    "        val_loss = compute_loss(X_val, y_val, weights, lambda_l1, lambda_l2)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_weights = weights.copy()\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping на епосі {epoch}\")\n",
    "            break\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Епоха {epoch+1}/{max_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nНайкраща модель на епосі {best_epoch} з Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return best_weights, train_losses, val_losses, best_epoch\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 6.4 Функція для прогнозування\n",
    "\n",
    "# %%\n",
    "def predict(X, weights, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Прогнозування класів\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(np.dot(X, weights))\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "def predict_proba(X, weights):\n",
    "    \"\"\"\n",
    "    Прогнозування ймовірностей\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, weights))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Навчання моделей\n",
    "# \n",
    "# ### 7.1 Стохастичний градієнтний спуск\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 60)\n",
    "print(\"НАВЧАННЯ: Стохастичний градієнтний спуск (SGD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Додавання intercept (стовпець одиниць)\n",
    "X_train_sgd = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_val_sgd = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "X_test_sgd = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "# Навчання\n",
    "weights_sgd, train_losses_sgd, val_losses_sgd, best_epoch_sgd = sgd(\n",
    "    X_train_sgd, y_train, X_val_sgd, y_val,\n",
    "    learning_rate=0.1,\n",
    "    max_epochs=200,\n",
    "    patience=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 7.2 Mini-batch градієнтний спуск\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"НАВЧАННЯ: Mini-batch градієнтний спуск\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Використовуємо ті ж дані з intercept\n",
    "weights_mb, train_losses_mb, val_losses_mb, best_epoch_mb = mini_batch_gd(\n",
    "    X_train_sgd, y_train, X_val_sgd, y_val,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=32,\n",
    "    max_epochs=200,\n",
    "    patience=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Візуалізація кривих навчання\n",
    "\n",
    "# %%\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# SGD\n",
    "axes[0].plot(train_losses_sgd, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses_sgd, label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(x=best_epoch_sgd, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_sgd})')\n",
    "axes[0].set_xlabel('Епоха', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "axes[0].set_title('Криві навчання: Стохастичний градієнтний спуск', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mini-batch\n",
    "axes[1].plot(train_losses_mb, label='Train Loss', linewidth=2)\n",
    "axes[1].plot(val_losses_mb, label='Validation Loss', linewidth=2)\n",
    "axes[1].axvline(x=best_epoch_mb, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_mb})')\n",
    "axes[1].set_xlabel('Епоха', fontsize=12)\n",
    "axes[1].set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "axes[1].set_title('Криві навчання: Mini-batch градієнтний спуск', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Аналіз кривих навчання\n",
    "# \n",
    "# **Інтерпретація:**\n",
    "# - Якщо обидві криві зменшуються і виходять на плато → навчання успішне\n",
    "# - Якщо валідаційна крива зростає, а тренувальна падає → перенавчання\n",
    "# - Якщо обидві криві залишаються високими → недонавчення\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Оцінка моделей на тестовому наборі\n",
    "# \n",
    "# ### 9.1 Функція для обчислення метрик\n",
    "\n",
    "# %%\n",
    "def evaluate_model(X, y, weights, model_name):\n",
    "    \"\"\"\n",
    "    Оцінка моделі та виведення метрик\n",
    "    \"\"\"\n",
    "    # Прогнози\n",
    "    y_pred = predict(X, weights)\n",
    "    y_proba = predict_proba(X, weights)\n",
    "    \n",
    "    # Метрики\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ОЦІНКА МОДЕЛІ: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 9.2 Оцінка SGD\n",
    "\n",
    "# %%\n",
    "results_sgd = evaluate_model(X_test_sgd, y_test, weights_sgd, \"Stochastic Gradient Descent\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 9.3 Оцінка Mini-batch\n",
    "\n",
    "# %%\n",
    "results_mb = evaluate_model(X_test_sgd, y_test, weights_mb, \"Mini-batch Gradient Descent\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Візуалізація Confusion Matrix\n",
    "\n",
    "# %%\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# SGD Confusion Matrix\n",
    "sns.heatmap(results_sgd['confusion_matrix'], annot=True, fmt='d', cmap='Blues', \n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix: SGD', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_xticklabels(['Abnormal', 'Normal'])\n",
    "axes[0].set_yticklabels(['Abnormal', 'Normal'])\n",
    "\n",
    "# Mini-batch Confusion Matrix\n",
    "sns.heatmap(results_mb['confusion_matrix'], annot=True, fmt='d', cmap='Greens', \n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('Confusion Matrix: Mini-batch GD', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_xticklabels(['Abnormal', 'Normal'])\n",
    "axes[1].set_yticklabels(['Abnormal', 'Normal'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Порівняльна таблиця метрик\n",
    "\n",
    "# %%\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Метод': 'SGD',\n",
    "        'Accuracy': f\"{results_sgd['accuracy']:.4f}\",\n",
    "        'Precision': f\"{results_sgd['precision']:.4f}\",\n",
    "        'Recall': f\"{results_sgd['recall']:.4f}\",\n",
    "        'F1-Score': f\"{results_sgd['f1']:.4f}\",\n",
    "        'Епох до збіжності': len(train_losses_sgd)\n",
    "    },\n",
    "    {\n",
    "        'Метод': 'Mini-batch GD',\n",
    "        'Accuracy': f\"{results_mb['accuracy']:.4f}\",\n",
    "        'Precision': f\"{results_mb['precision']:.4f}\",\n",
    "        'Recall': f\"{results_mb['recall']:.4f}\",\n",
    "        'F1-Score': f\"{results_mb['f1']:.4f}\",\n",
    "        'Епох до збіжності': len(train_losses_mb)\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПОРІВНЯЛЬНА ТАБЛИЦЯ МЕТРИК\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Додатково: Регуляризація (L1 та L2)\n",
    "# \n",
    "# ### 12.1 Навчання з L2 регуляризацією (Ridge)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"НАВЧАННЯ З L2 РЕГУЛЯРИЗАЦІЄЮ (Ridge)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weights_l2, train_losses_l2, val_losses_l2, best_epoch_l2 = mini_batch_gd(\n",
    "    X_train_sgd, y_train, X_val_sgd, y_val,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=32,\n",
    "    max_epochs=200,\n",
    "    lambda_l2=0.01,  # Коефіцієнт L2 регуляризації\n",
    "    patience=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results_l2 = evaluate_model(X_test_sgd, y_test, weights_l2, \"Mini-batch GD with L2\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 12.2 Навчання з L1 регуляризацією (Lasso)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"НАВЧАННЯ З L1 РЕГУЛЯРИЗАЦІЄЮ (Lasso)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weights_l1, train_losses_l1, val_losses_l1, best_epoch_l1 = mini_batch_gd(\n",
    "    X_train_sgd, y_train, X_val_sgd, y_val,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=32,\n",
    "    max_epochs=200,\n",
    "    lambda_l1=0.01,  # Коефіцієнт L1 регуляризації\n",
    "    patience=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results_l1 = evaluate_model(X_test_sgd, y_test, weights_l1, \"Mini-batch GD with L1\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 12.3 Порівняння всіх моделей\n",
    "\n",
    "# %%\n",
    "comparison_full_df = pd.DataFrame([\n",
    "    {\n",
    "        'Метод': 'SGD',\n",
    "        'Accuracy': f\"{results_sgd['accuracy']:.4f}\",\n",
    "        'Precision': f\"{results_sgd['precision']:.4f}\",\n",
    "        'Recall': f\"{results_sgd['recall']:.4f}\",\n",
    "        'F1-Score': f\"{results_sgd['f1']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Метод': 'Mini-batch GD',\n",
    "        'Accuracy': f\"{results_mb['accuracy']:.4f}\",\n",
    "        'Precision': f\"{results_mb['precision']:.4f}\",\n",
    "        'Recall': f\"{results_mb['recall']:.4f}\",\n",
    "        'F1-Score': f\"{results_mb['f1']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Метод': 'Mini-batch GD + L2',\n",
    "        'Accuracy': f\"{results_l2['accuracy']:.4f}\",\n",
    "        'Precision': f\"{results_l2['precision']:.4f}\",\n",
    "        'Recall': f\"{results_l2['recall']:.4f}\",\n",
    "        'F1-Score': f\"{results_l2['f1']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Метод': 'Mini-batch GD + L1',\n",
    "        'Accuracy': f\"{results_l1['accuracy']:.4f}\",\n",
    "        'Precision': f\"{results_l1['precision']:.4f}\",\n",
    "        'Recall': f\"{results_l1['recall']:.4f}\",\n",
    "        'F1-Score': f\"{results_l1['f1']:.4f}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПОВНА ПОРІВНЯЛЬНА ТАБЛИЦЯ ВСІХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_full_df.to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 12.4 Візуалізація кривих навчання для регуляризованих моделей\n",
    "\n",
    "# %%\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# L2 Regularization\n",
    "axes[0].plot(train_losses_l2, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses_l2, label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(x=best_epoch_l2, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_l2})')\n",
    "axes[0].set_xlabel('Епоха', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "axes[0].set_title('Криві навчання: L2 Регуляризація (Ridge)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# L1 Regularization\n",
    "axes[1].plot(train_losses_l1, label='Train Loss', linewidth=2)\n",
    "axes[1].plot(val_losses_l1, label='Validation Loss', linewidth=2)\n",
    "axes[1].axvline(x=best_epoch_l1, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_l1})')\n",
    "axes[1].set_xlabel('Епоха', fontsize=12)\n",
    "axes[1].set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12)\n",
    "axes[1].set_title('Криві навчання: L1 Регуляризація (Lasso)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Аналіз помилкових класифікацій\n",
    "\n",
    "# %%\n",
    "# Виконаємо детальний аналіз на прикладі найкращої моделі (наприклад, Mini-batch)\n",
    "y_pred_mb = predict(X_test_sgd, weights_mb)\n",
    "\n",
    "# Знаходимо індекси помилково класифікованих прикладів\n",
    "false_positives = np.where((y_test == 0) & (y_pred_mb == 1))[0]\n",
    "false_negatives = np.where((y_test == 1) & (y_pred_mb == 0))[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"АНАЛІЗ ПОМИЛКОВИХ КЛАСИФІКАЦІЙ\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nКількість False Positives (передбачили Normal, насправді Abnormal): {len(false_positives)}\")\n",
    "print(f\"Кількість False Negatives (передбачили Abnormal, насправді Normal): {len(false_negatives)}\")\n",
    "\n",
    "# Confusion Matrix детальний розбір\n",
    "cm = results_mb['confusion_matrix']\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nДетальний розбір Confusion Matrix:\")\n",
    "print(f\"True Negatives (правильно передбачені Abnormal):  {tn}\")\n",
    "print(f\"False Positives (помилково передбачені Normal):   {fp}\")\n",
    "print(f\"False Negatives (помилково передбачені Abnormal): {fn}\")\n",
    "print(f\"True Positives (правильно передбачені Normal):    {tp}\")\n",
    "\n",
    "# Частота помилок\n",
    "total = tn + fp + fn + tp\n",
    "print(f\"\\nВідсотки:\")\n",
    "print(f\"True Negative Rate:  {tn/total*100:.2f}%\")\n",
    "print(f\"False Positive Rate: {fp/total*100:.2f}%\")\n",
    "print(f\"False Negative Rate: {fn/total*100:.2f}%\")\n",
    "print(f\"True Positive Rate:  {tp/total*100:.2f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Інтерпретація помилок\n",
    "# \n",
    "# **False Positives (FP):** Модель передбачила нормальний результат тесту, але насправді він був аномальним.\n",
    "# - Це може бути небезпечно у медичному контексті, оскільки пацієнт з проблемою може не отримати необхідного лікування.\n",
    "# \n",
    "# **False Negatives (FN):** Модель передбачила аномальний результат, але насправді тест був нормальним.\n",
    "# - Це призводить до зайвих обстежень та тривоги пацієнта, але є менш критичним, ніж FP.\n",
    "# \n",
    "# **Можливі причини помилок:**\n",
    "# 1. Перетин розподілів класів у просторі ознак\n",
    "# 2. Недостатня інформативність деяких ознак\n",
    "# 3. Наявність викидів або аномалій у даних\n",
    "# 4. Дисбаланс класів (якщо присутній)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. Візуалізація важливості ознак\n",
    "\n",
    "# %%\n",
    "# Аналіз ваг моделі (абсолютні значення показують важливість)\n",
    "feature_names = ['Intercept'] + X_encoded.columns.tolist()\n",
    "weights_abs = np.abs(weights_mb)\n",
    "\n",
    "# Топ-15 найважливіших ознак\n",
    "top_n = 15\n",
    "top_indices = np.argsort(weights_abs)[-top_n:][::-1]\n",
    "top_features = [feature_names[i] for i in top_indices]\n",
    "top_weights = weights_abs[top_indices]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_features)), top_weights, color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Абсолютне значення ваги', fontsize=12)\n",
    "plt.title(f'Топ-{top_n} найважливіших ознак (Mini-batch GD)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. Висновки та рекомендації\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 15.1 Порівняння методів оптимізації\n",
    "# \n",
    "# **Стохастичний градієнтний спуск (SGD):**\n",
    "# - ✅ Швидше оновлює ваги (після кожного прикладу)\n",
    "# - ✅ Може допомогти уникнути локальних мінімумів через шум\n",
    "# - ❌ Більш нестабільна збіжність (коливання функції втрат)\n",
    "# - ❌ Вимагає ретельного налаштування швидкості навчання\n",
    "# \n",
    "# **Mini-batch градієнтний спуск:**\n",
    "# - ✅ Баланс між швидкістю та стабільністю\n",
    "# - ✅ Більш стабільна збіжність\n",
    "# - ✅ Ефективне використання векторизації\n",
    "# - ✅ Краще підходить для великих наборів даних\n",
    "# - ❌ Потребує налаштування розміру батчу\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 15.2 Вплив регуляризації\n",
    "# \n",
    "# **L2 Регуляризація (Ridge):**\n",
    "# - Зменшує всі ваги пропорційно\n",
    "# - Допомагає запобігти перенавчанню\n",
    "# - Зберігає всі ознаки у моделі\n",
    "# - Корисна при мультиколінеарності\n",
    "# \n",
    "# **L1 Регуляризація (Lasso):**\n",
    "# - Може зменшити деякі ваги до нуля\n",
    "# - Виконує відбір ознак (feature selection)\n",
    "# - Створює розріджені моделі\n",
    "# - Корисна при великій кількості нерелевантних ознак\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 15.3 Загальні висновки\n",
    "# \n",
    "# 1. **Якість моделі:** Модель показує [залежить від ваших даних] точність передбачення\n",
    "# \n",
    "# 2. **Найкращий метод:** [Порівняйте метрики] показав найкращі результати\n",
    "# \n",
    "# 3. **Регуляризація:** [Оцініть, чи покращила регуляризація результати]\n",
    "# \n",
    "# 4. **Проблемні області:**\n",
    "#    - Модель частіше помиляється на [опишіть патерни помилок]\n",
    "#    - Можливо потрібно більше даних або інженерія ознак\n",
    "# \n",
    "# 5. **Рекомендації для покращення:**\n",
    "#    - Додати більше релевантних ознак\n",
    "#    - Спробувати інші порогові значення класифікації\n",
    "#    - Розглянути ансамблеві методи\n",
    "#    - Провести балансування класів (якщо є дисбаланс)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 16. Додатковий аналіз: ROC-крива та AUC\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Обчислення ROC-кривої для всіх моделей\n",
    "models = [\n",
    "    ('SGD', weights_sgd),\n",
    "    ('Mini-batch GD', weights_mb),\n",
    "    ('Mini-batch + L2', weights_l2),\n",
    "    ('Mini-batch + L1', weights_l1)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, weights in models:\n",
    "    y_proba = predict_proba(X_test_sgd, weights)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC-криві для всіх моделей', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Інтерпретація ROC-кривої\n",
    "# \n",
    "# - **AUC (Area Under Curve)** показує загальну якість моделі\n",
    "# - AUC = 1.0: ідеальна модель\n",
    "# - AUC = 0.5: випадкове передбачення\n",
    "# - Чим вище крива над діагоналлю, тим краща модель\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 17. Збереження найкращої моделі\n",
    "\n",
    "# %%\n",
    "# Зберігаємо найкращу модель\n",
    "best_model_name = 'Mini-batch GD'  # Змініть на найкращу за вашими результатами\n",
    "best_weights = weights_mb\n",
    "\n",
    "# Зберігаємо параметри для майбутнього використання\n",
    "model_params = {\n",
    "    'weights': best_weights,\n",
    "    'feature_names': feature_names,\n",
    "    'X_mean': X_mean,\n",
    "    'X_std': X_std,\n",
    "    'model_type': best_model_name\n",
    "}\n",
    "\n",
    "# Можна зберегти за допомогою pickle або numpy\n",
    "np.save('best_model_weights.npy', best_weights)\n",
    "print(f\"\\n✅ Найкраща модель ({best_model_name}) збережена у файл 'best_model_weights.npy'\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 18. Функція для передбачення на нових даних\n",
    "\n",
    "# %%\n",
    "def predict_new_data(new_data_df, weights, X_mean, X_std, feature_names):\n",
    "    \"\"\"\n",
    "    Функція для передбачення на нових даних\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_data_df : DataFrame\n",
    "        Нові дані у форматі pandas DataFrame\n",
    "    weights : array\n",
    "        Навчені ваги моделі\n",
    "    X_mean, X_std : array\n",
    "        Параметри нормалізації з тренувального набору\n",
    "    feature_names : list\n",
    "        Список назв ознак\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : array\n",
    "        Передбачені класи (0 або 1)\n",
    "    probabilities : array\n",
    "        Ймовірності належності до класу 1\n",
    "    \"\"\"\n",
    "    # Підготовка даних (той самий препроцесинг, що і для тренувальних даних)\n",
    "    X_new = pd.get_dummies(new_data_df, drop_first=True)\n",
    "    \n",
    "    # Переконуємося, що всі колонки присутні\n",
    "    for col in feature_names[1:]:  # Пропускаємо Intercept\n",
    "        if col not in X_new.columns:\n",
    "            X_new[col] = 0\n",
    "    \n",
    "    # Вибираємо лише потрібні колонки у правильному порядку\n",
    "    X_new = X_new[feature_names[1:]]\n",
    "    \n",
    "    # Нормалізація\n",
    "    X_new_normalized = (X_new.values - X_mean) / X_std\n",
    "    \n",
    "    # Додаємо intercept\n",
    "    X_new_with_intercept = np.c_[np.ones(X_new_normalized.shape[0]), X_new_normalized]\n",
    "    \n",
    "    # Передбачення\n",
    "    probabilities = predict_proba(X_new_with_intercept, weights)\n",
    "    predictions = (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "# Приклад використання (коментар, розкоментуйте для використання):\n",
    "# new_patient = pd.DataFrame({...})  # Дані нового пацієнта\n",
    "# pred, prob = predict_new_data(new_patient, best_weights, X_mean, X_std, feature_names)\n",
    "# print(f\"Передбачення: {'Normal' if pred[0] == 1 else 'Abnormal'}\")\n",
    "# print(f\"Ймовірність Normal: {prob[0]:.2%}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ## КІНЕЦЬ АНАЛІЗУ\n",
    "# \n",
    "# **Автори:** [Ваше ім'я]\n",
    "# \n",
    "# **Дата:** [Поточна дата]\n",
    "# \n",
    "# **Версія:** 1.0\n",
    "# \n",
    "# ---\n",
    "# \n",
    "# ### Підсумок виконаної роботи:\n",
    "# \n",
    "# ✅ Завантажено та проаналізовано медичні дані\n",
    "# \n",
    "# ✅ Виконано препроцесинг даних (one-hot encoding, нормалізація)\n",
    "# \n",
    "# ✅ Реалізовано логістичну регресію з нуля\n",
    "# \n",
    "# ✅ Імплементовано SGD та Mini-batch градієнтний спуск\n",
    "# \n",
    "# ✅ Додано L1 та L2 регуляризацію\n",
    "# \n",
    "# ✅ Побудовано криві навчання та ROC-криві\n",
    "# \n",
    "# ✅ Виконано детальну оцінку моделей\n",
    "# \n",
    "# ✅ Проаналізовано помилки класифікації\n",
    "# \n",
    "# ✅ Створено функцію для передбачення на нових даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d322a20-7539-4031-b121-b6efe580dd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8928a4-a2d4-4245-ba15-95b0d860ad4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
